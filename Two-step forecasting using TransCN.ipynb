{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f41a1a-b8b8-4507-b010-0ffa43a01f88",
   "metadata": {},
   "source": [
    "# Two Step Forecasting for weather and kWh delivered using TransCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f6b67c-f5fd-4cd1-ad16-f5fdcef55a48",
   "metadata": {},
   "source": [
    "In this model, I would be implementing a two-step forecast that would first predict the weather for a time in the future and then use the predicted weather data as an input for the kWh Delivered forecasting for the model.\n",
    "\n",
    "In summary, I will be exploring three methods:\n",
    "\n",
    "1. Using historical weather data from only one charging station to predict kWhDelivered\n",
    "2. Using historical weather data from 54 charging stations with cummulative kWhDelivered added based on stationID\n",
    "3. Using historical data for each 54 charging stations using Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e7e58b-4123-4a93-a038-29a3382d5de3",
   "metadata": {},
   "source": [
    "# First step Forecasting\n",
    "\n",
    "Predicting future weather data and testing the accuracy of the GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11042e27-89d9-481c-80e0-5f6a4149e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5b79443-15f8-4bf5-804a-44ce26e20005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from smape import smape\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential, load_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Add, Input, Conv1D, MaxPooling1D, Dropout, Flatten, Dense, LayerNormalization, MultiHeadAttention\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "# Make the process reproducible using a seed\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# Limit TensorFlow to a single thread\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"1\"\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53658fb7-5f69-4954-8c89-4931c65733e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weather data from the weather data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "926e4e19-911b-4e26-b4b2-e93813dbef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './caltech_weather_data'\n",
    "data_frames = []\n",
    "\n",
    "# Function to safely extract data from JSON\n",
    "def extract_hourly_data(data):\n",
    "    historical_data = data.get('historical', {})\n",
    "    for date, details in historical_data.items():\n",
    "        hourly_data = details.get('hourly', [])\n",
    "        for hour_data in hourly_data:\n",
    "            hour_data['date'] = date\n",
    "            data_frames.append(hour_data)\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            with open(file_path) as file:\n",
    "                data = json.load(file)\n",
    "                extract_hourly_data(data)\n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            print(f\"Error processing file {file_name}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(data_frames)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.sort_values('date', inplace=True)\n",
    "\n",
    "\n",
    "weather_df = pd.DataFrame(data_frames)\n",
    "weather_df['date'] = pd.to_datetime(weather_df['date'])\n",
    "weather_df.sort_values('date', inplace=True)\n",
    "\n",
    "# Select relevant features\n",
    "weather_features = ['temperature', 'wind_speed', 'pressure', 'humidity', 'precip']\n",
    "weather_df = weather_df[weather_features + ['date']]\n",
    "\n",
    "# Aggregate weather data by day (since kWh data is monthly, daily aggregation should be sufficient)\n",
    "weather_daily_df = weather_df.groupby('date').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df914035-5a69-48dc-bb6d-1053b1fe60c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7774a954-8d8f-42f5-ab19-004abcb66a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['temperature', 'wind_speed', 'pressure', 'humidity', 'precip']\n",
    "df = df[features + ['date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ddeba6-b1f0-41c3-b252-6c317fd1d5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data and split into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "688327cd-baa6-4267-9f9d-7743e1dada39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30232558, 0.1875    , 0.5       , 0.4787234 , 0.        ],\n",
       "       [0.27906977, 0.125     , 0.46875   , 0.58510638, 0.        ],\n",
       "       [0.27906977, 0.1875    , 0.46875   , 0.56382979, 0.        ],\n",
       "       ...,\n",
       "       [0.37209302, 0.0625    , 0.4375    , 0.62765957, 0.        ],\n",
       "       [0.58139535, 0.46875   , 0.40625   , 0.30851064, 0.        ],\n",
       "       [0.37209302, 0.0625    , 0.4375    , 0.61702128, 0.        ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(df[features])\n",
    "\n",
    "# Create sequences for GRU\n",
    "sequence_length = 30\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(sequence_length, len(scaled_data)):\n",
    "    X.append(scaled_data[i-sequence_length:i])\n",
    "    y.append(scaled_data[i])\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_size = int(X.shape[0] * 0.9)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e5ebbf-a08c-4c74-b05b-677fc2965ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a TCN using the data from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "26d02cb5-0051-4190-baee-e5a6e04eb4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim, ff_dim, dropout_rate):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation='relu'),\n",
    "            Dense(key_dim)  # Match the output dimension with key_dim for residual connection\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Ensure the layer is built with the correct input shape\n",
    "        super(TransformerEncoder, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Self-attention\n",
    "        attn_output = self.attention(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)  # Residual connection\n",
    "        \n",
    "        # Feed-forward network\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)  # Residual connection\n",
    "\n",
    "# Define the TransCN model\n",
    "def build_transcn_model(input_shape, num_heads, key_dim, ff_dim, dropout_rate, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Convolutional Layers\n",
    "    x = Conv1D(filters=64, kernel_size=3, activation='relu')(inputs)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Transformer Encoder Block\n",
    "    x = TransformerEncoder(num_heads=num_heads, key_dim=key_dim, ff_dim=ff_dim, dropout_rate=dropout_rate)(x)\n",
    "    \n",
    "    # Flatten and Dense Layers\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(num_classes)(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6da39830-1c7a-48ad-a7c1-bf80c5c02453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model and check for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d247443-6cb3-4333-b22c-b2b6da993cc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling TransformerEncoder.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'transformer_encoder_16' (of type TransformerEncoder). Either the `TransformerEncoder.call()` method is incorrect, or you need to implement the `TransformerEncoder.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nDimensions must be equal, but are 64 and 5 for '{{node add_1}} = AddV2[T=DT_FLOAT](layer_normalization_32_1/add_2, sequential_16_1/dense_39_1/Add)' with input shapes: [?,14,64], [?,14,5].\u001b[0m\n\nArguments received by TransformerEncoder.call():\n  • args=('<KerasTensor shape=(None, 14, 64), dtype=float32, sparse=False, name=keras_tensor_130>',)\n  • kwargs={'training': 'False'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# For regression, set to 1\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Build and compile the model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_transcn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mff_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Inverse scale the predictions and actual values\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[51], line 39\u001b[0m, in \u001b[0;36mbuild_transcn_model\u001b[0;34m(input_shape, num_heads, key_dim, ff_dim, dropout_rate, num_classes)\u001b[0m\n\u001b[1;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m Dropout(\u001b[38;5;241m0.2\u001b[39m)(x)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Transformer Encoder Block\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mTransformerEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mff_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mff_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_rate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Flatten and Dense Layers\u001b[39;00m\n\u001b[1;32m     42\u001b[0m x \u001b[38;5;241m=\u001b[39m Flatten()(x)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[51], line 27\u001b[0m, in \u001b[0;36mTransformerEncoder.call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     25\u001b[0m ffn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(out1)\n\u001b[1;32m     26\u001b[0m ffn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(ffn_output, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm2(\u001b[43mout1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mffn_output\u001b[49m)\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling TransformerEncoder.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'transformer_encoder_16' (of type TransformerEncoder). Either the `TransformerEncoder.call()` method is incorrect, or you need to implement the `TransformerEncoder.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nDimensions must be equal, but are 64 and 5 for '{{node add_1}} = AddV2[T=DT_FLOAT](layer_normalization_32_1/add_2, sequential_16_1/dense_39_1/Add)' with input shapes: [?,14,64], [?,14,5].\u001b[0m\n\nArguments received by TransformerEncoder.call():\n  • args=('<KerasTensor shape=(None, 14, 64), dtype=float32, sparse=False, name=keras_tensor_130>',)\n  • kwargs={'training': 'False'}"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "input_shape = (30, 5)  # Adjusted input shape to match your data\n",
    "num_heads = 4\n",
    "key_dim = 5  # Adjusted key dimension to match the input's last dimension\n",
    "ff_dim = 64  # Feed-forward network dimension\n",
    "dropout_rate = 0.2\n",
    "num_classes = 1  # For regression, set to 1\n",
    "\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_transcn_model(input_shape, num_heads, key_dim, ff_dim, dropout_rate, num_classes)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse scale the predictions and actual values\n",
    "y_test_inverse = scaler.inverse_transform(y_test)\n",
    "y_pred_inverse = scaler.inverse_transform(y_pred)\n",
    "\n",
    "# Evaluate the model for each feature\n",
    "mae_per_feature = []\n",
    "mse_per_feature = []\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    mae = mean_absolute_error(y_test_inverse[:, i], y_pred_inverse[:, i])\n",
    "    mse = mean_squared_error(y_test_inverse[:, i], y_pred_inverse[:, i])\n",
    "    mae_per_feature.append(mae)\n",
    "    mse_per_feature.append(mse)\n",
    "    print(f'{feature.capitalize()} - Mean Absolute Error: {mae}')\n",
    "    print(f'{feature.capitalize()} - Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c049d185-2d56-4776-9704-05af9d1234a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46a9caf1-7ae2-42b7-84c9-f73b427233bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test_inverse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(features):\n\u001b[1;32m      4\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;28mlen\u001b[39m(features), \u001b[38;5;241m1\u001b[39m, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m][train_size\u001b[38;5;241m+\u001b[39msequence_length:], \u001b[43my_test_inverse\u001b[49m[:, i], color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActual \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;241m.\u001b[39mcapitalize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m][train_size\u001b[38;5;241m+\u001b[39msequence_length:], y_pred_inverse[:, i], color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;241m.\u001b[39mcapitalize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;241m.\u001b[39mcapitalize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Prediction\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test_inverse' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Actual vs Predicted for all features\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i, feature in enumerate(features):\n",
    "    plt.subplot(len(features), 1, i+1)\n",
    "    plt.plot(df['date'][train_size+sequence_length:], y_test_inverse[:, i], color='blue', label=f'Actual {feature.capitalize()}')\n",
    "    plt.plot(df['date'][train_size+sequence_length:], y_pred_inverse[:, i], color='red', label=f'Predicted {feature.capitalize()}')\n",
    "    plt.title(f'{feature.capitalize()} Prediction')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(feature.capitalize())\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147a2e67-3306-4eda-940f-c39a97c68c3d",
   "metadata": {},
   "source": [
    "# Second step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0804bfba-e54e-425d-83dd-316a1c314359",
   "metadata": {},
   "source": [
    "Using the forecasted weather data to predict future kWh delivered by electric vehicle charging stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06758b2-ce5e-4a64-af8a-be96030e1388",
   "metadata": {},
   "source": [
    "## Method 1: Using historical weather data from only one charging station to predict kWhDelivered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1f6ca3-44c3-4723-a345-8463bed0a949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model data for single charging station\n",
    "data = pd.read_csv('./caltech_model_data/2-39-123-23.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd77b50-9dab-47b5-8c0a-32d5146b572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the month to datetime format and set it as the index\n",
    "data['Month'] = pd.to_datetime(data['month'])\n",
    "data.set_index('Month', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecba209-1af4-4e63-9e71-3c91fd4c0048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features (weather data) and target variable (kWhDelivered)\n",
    "features = ['MinTemp', 'MaxTemp', 'AvgTemp', 'AvgPrecipitation', 'AvgHumidity', 'AvgWindSpeed']\n",
    "target = 'kWhDelivered'\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(data[features])\n",
    "\n",
    "# Scale the target variable (kWhDelivered) separately\n",
    "target_scaler = MinMaxScaler()\n",
    "scaled_target = target_scaler.fit_transform(data[[target]])  # Use double brackets to keep it as a DataFrame\n",
    "data['Scaled_kWhDelivered'] = scaled_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8491ac2e-af23-4eab-a95f-6bc6752db975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the scaled features back to a DataFrame\n",
    "scaled_features_df = pd.DataFrame(scaled_features, columns=features, index=data.index)\n",
    "\n",
    "# Combine the scaled features with the scaled target variable\n",
    "scaled_data = pd.concat([scaled_features_df, data['Scaled_kWhDelivered']], axis=1)\n",
    "\n",
    "# Create a lagged feature for kWhDelivered\n",
    "def create_sequences(data, sequence_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        seq = data.iloc[i:i+sequence_length][features].values\n",
    "        label = data.iloc[i+sequence_length]['Scaled_kWhDelivered']\n",
    "        sequences.append(seq)\n",
    "        targets.append(label)\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Set the sequence length (e.g., 1 for 1 months of data)\n",
    "sequence_length = 1\n",
    "\n",
    "# Create sequences and targets\n",
    "X, y = create_sequences(scaled_data, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6e8a6f-d7db-401a-9bb2-6faa6edc5da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (use all except the last one for training)\n",
    "X_train, X_test = X[:-1], X[-1:]\n",
    "y_train, y_test = y[:-1], y[-1:]\n",
    "\n",
    "# Extract the months corresponding to the training and test data\n",
    "months_train = data.index[sequence_length:len(X_train)+sequence_length]\n",
    "months_test = data.index[-len(X_test):]\n",
    "\n",
    "# Define the GRU model\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = Sequential()\n",
    "model.add(Input(shape=input_shape))\n",
    "model.add(GRU(units=50, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GRU(units=50))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=MeanSquaredError())\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ca5a75-3bbc-4a69-a260-eb3618515a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the values for both training and test sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Reshape y_train, y_test, y_train_pred, and y_test_pred for inverse transformation\n",
    "y_train_scaled = y_train.reshape(-1, 1)\n",
    "y_test_scaled = y_test.reshape(-1, 1)\n",
    "y_train_pred_scaled = y_train_pred.reshape(-1, 1)\n",
    "y_test_pred_scaled = y_test_pred.reshape(-1, 1)\n",
    "\n",
    "# Inverse transform the predictions and actual values using the target scaler\n",
    "y_train_inversed = target_scaler.inverse_transform(y_train_scaled)\n",
    "y_test_inversed = target_scaler.inverse_transform(y_test_scaled)\n",
    "y_train_pred_inversed = target_scaler.inverse_transform(y_train_pred_scaled)\n",
    "y_test_pred_inversed = target_scaler.inverse_transform(y_test_pred_scaled)\n",
    "\n",
    "# Plot the actual vs predicted values against the month for both training and test sets\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot training data\n",
    "plt.plot(months_train, y_train_inversed, label='Actual kWhDelivered (Training)', marker='o', color='blue')\n",
    "plt.plot(months_train, y_train_pred_inversed, label='Predicted kWhDelivered (Training)', marker='x', color='orange')\n",
    "\n",
    "# Plot test data\n",
    "plt.plot(months_test, y_test_inversed, label='Actual kWhDelivered (Test)', marker='o', color='green')\n",
    "plt.plot(months_test, y_test_pred_inversed, label='Predicted kWhDelivered (Test)', marker='x', color='red')\n",
    "\n",
    "plt.title('Actual vs Predicted kWhDelivered (Training and Test)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('kWhDelivered')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf5e483-a598-4a24-b1a7-0684b75cd064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics\n",
    "mse = mean_squared_error(y_test_inversed, y_test_pred_inversed)\n",
    "mae = mean_absolute_error(y_test_inversed, y_test_pred_inversed)\n",
    "smape_value = smape(y_test_inversed, y_test_pred_inversed)\n",
    "\n",
    "print(f'Mean Squared Error: {round(mse,2)}')\n",
    "print(f'Mean Absolute Error: {round(mae,2)}')\n",
    "print(f'Symmetric Mean Absolute Percentage Error: {round(smape_value,2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac7060b-4c97-4c39-ad81-e2a2c7b2d8bc",
   "metadata": {},
   "source": [
    "## Method 2: Making Predictions with GRU using historical weather data from 54 charging stations with cummulative kWhDelivered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6018fc0-ca9d-4e60-b6a2-c069b9c48f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './caltech_model_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315214b9-687d-40fa-b816-641a0c700110",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.DataFrame()\n",
    "\n",
    "\n",
    "# Loop through each CSV file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        # Load the data from the CSV file\n",
    "        data = pd.read_csv(filepath)\n",
    "        # Append the data to the combined DataFrame\n",
    "        combined_data = pd.concat([combined_data, data], axis=0)\n",
    "\n",
    "summed_data = combined_data.groupby('month').agg({\n",
    "    'kWhDelivered': 'sum',\n",
    "    'MinTemp': 'first',\n",
    "    'MaxTemp': 'first',\n",
    "    'AvgTemp': 'first',\n",
    "    'AvgPrecipitation': 'first',\n",
    "    'AvgHumidity': 'first',\n",
    "    'AvgWindSpeed': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Print or save the result\n",
    "summed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3994f7c-d116-4948-a846-a7889d0fc295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the month to datetime format and set it as the index\n",
    "summed_data['Month'] = pd.to_datetime(summed_data['month'])\n",
    "summed_data.set_index('Month', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cae7242-3e89-43f9-97f9-c48d26c0fdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features (weather data) and target variable (kWhDelivered)\n",
    "features = ['MinTemp', 'MaxTemp', 'AvgTemp', 'AvgPrecipitation', 'AvgHumidity', 'AvgWindSpeed']\n",
    "target = 'kWhDelivered'\n",
    "\n",
    "# Scale the features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(summed_data[features])\n",
    "\n",
    "# Scale the target variable (kWhDelivered) separately\n",
    "target_scaler = MinMaxScaler()\n",
    "scaled_target = target_scaler.fit_transform(summed_data[[target]])  # Use double brackets to keep it as a DataFrame\n",
    "summed_data['Scaled_kWhDelivered'] = scaled_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a15114e-ed22-4215-9b72-78da71eba1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the scaled features back to a DataFrame\n",
    "scaled_features_df = pd.DataFrame(scaled_features, columns=features, index=summed_data.index)\n",
    "\n",
    "# Combine the scaled features with the scaled target variable\n",
    "scaled_data = pd.concat([scaled_features_df, summed_data['Scaled_kWhDelivered']], axis=1)\n",
    "\n",
    "# Create a lagged feature for kWhDelivered\n",
    "def create_sequences(summed_data, sequence_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(summed_data) - sequence_length):\n",
    "        seq = summed_data.iloc[i:i+sequence_length][features].values\n",
    "        label = summed_data.iloc[i+sequence_length]['Scaled_kWhDelivered']\n",
    "        sequences.append(seq)\n",
    "        targets.append(label)\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Set the sequence length (e.g., 3 for 3 months of data)\n",
    "sequence_length = 3\n",
    "\n",
    "# Create sequences and targets\n",
    "X, y = create_sequences(scaled_data, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769120d5-c5a3-4ba8-82b9-29e374f0c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets (use all except the last one for training)\n",
    "X_train, X_test = X[:-1], X[-1:]\n",
    "y_train, y_test = y[:-1], y[-1:]\n",
    "\n",
    "# Extract the months corresponding to the training and test data\n",
    "months_train = summed_data.index[sequence_length:len(X_train)+sequence_length]\n",
    "months_test = summed_data.index[-len(X_test):]\n",
    "\n",
    "# Define the GRU model\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = Sequential()\n",
    "model.add(Input(shape=input_shape))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb784201-083c-4e3d-b2b1-8dd311c9e9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the values for both training and test sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Reshape y_train, y_test, y_train_pred, and y_test_pred for inverse transformation\n",
    "y_train_scaled = y_train.reshape(-1, 1)\n",
    "y_test_scaled = y_test.reshape(-1, 1)\n",
    "y_train_pred_scaled = y_train_pred.reshape(-1, 1)\n",
    "y_test_pred_scaled = y_test_pred.reshape(-1, 1)\n",
    "\n",
    "# Inverse transform the predictions and actual values using the target scaler\n",
    "y_train_inversed = target_scaler.inverse_transform(y_train_scaled)\n",
    "y_test_inversed = target_scaler.inverse_transform(y_test_scaled)\n",
    "y_train_pred_inversed = target_scaler.inverse_transform(y_train_pred_scaled)\n",
    "y_test_pred_inversed = target_scaler.inverse_transform(y_test_pred_scaled)\n",
    "\n",
    "# Plot the actual vs predicted values against the month for both training and test sets\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot training data\n",
    "plt.plot(months_train, y_train_inversed, label='Actual kWhDelivered (Training)', marker='o', color='blue')\n",
    "plt.plot(months_train, y_train_pred_inversed, label='Predicted kWhDelivered (Training)', marker='x', color='orange')\n",
    "\n",
    "# Plot test data\n",
    "plt.plot(months_test, y_test_inversed, label='Actual kWhDelivered (Test)', marker='o', color='green')\n",
    "plt.plot(months_test, y_test_pred_inversed, label='Predicted kWhDelivered (Test)', marker='x', color='red')\n",
    "\n",
    "plt.title('Actual vs Predicted kWhDelivered (Training and Test)')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('kWhDelivered')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acea647b-7bef-4664-9185-cd8969e28191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics\n",
    "mse = mean_squared_error(y_test_inversed, y_test_pred_inversed)\n",
    "mae = mean_absolute_error(y_test_inversed, y_test_pred_inversed)\n",
    "smape_value = smape(y_test_inversed, y_test_pred_inversed)\n",
    "\n",
    "print(f'Mean Squared Error: {round(mse,2)}')\n",
    "print(f'Mean Absolute Error: {round(mae,2)}')\n",
    "print(f'Symmetric Mean Absolute Percentage Error: {round(smape_value,2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b38f414-14a7-4947-97b5-fb6066896123",
   "metadata": {},
   "source": [
    "## Method 3: Making Predictions with GRU using historical weather data for each 54 charging stations with Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b542f8-03ed-4764-85a8-25e33c4034a7",
   "metadata": {},
   "source": [
    "### With Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8db434-020f-4dfb-aaeb-c7832c1b3b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the models and metrics\n",
    "models = {}\n",
    "metrics = {}\n",
    "\n",
    "# Directory containing the CSV files\n",
    "directory = \"./caltech_model_data\"  # Replace with your directory path\n",
    "\n",
    "# List all files in the directory\n",
    "filenames = sorted([filename for filename in os.listdir(directory) if filename.endswith('.csv')])\n",
    "\n",
    "# Placeholder to store the first trained model for Transfer Learning\n",
    "previous_model = None\n",
    "\n",
    "def prepare_data(features, target):\n",
    "    # For window_size = 1, each sample is its own sequence\n",
    "    return features, target\n",
    "\n",
    "def build_gru_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(GRU(units=50, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(units=50))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    return model\n",
    "\n",
    "def inverse_transform(scaler, data):\n",
    "    return scaler.inverse_transform(data.reshape(-1, 1)).flatten()\n",
    "\n",
    "def create_sequences(data, sequence_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        seq = data.iloc[i:i+sequence_length][features].values\n",
    "        label = data.iloc[i+sequence_length]['Scaled_kWhDelivered']\n",
    "        sequences.append(seq)\n",
    "        targets.append(label)\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Loop through each CSV file in the directory\n",
    "for filename in filenames:\n",
    "    try:\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        # Load the data from the CSV file\n",
    "        data = pd.read_csv(filepath)\n",
    "        \n",
    "        # Convert the month to datetime format and set it as the index\n",
    "        data['Month'] = pd.to_datetime(data['month'])\n",
    "        data.set_index('Month', inplace=True)\n",
    "    \n",
    "        # Select the features (weather data) and target variable (kWhDelivered)\n",
    "        stationID = filename.split('.')[0]  # Example: If filename is 'station1.csv'\n",
    "        features = ['MinTemp', 'MaxTemp', 'AvgTemp', 'AvgPrecipitation', 'AvgHumidity', 'AvgWindSpeed']\n",
    "        target = 'kWhDelivered'\n",
    "    \n",
    "        # Scale the features\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled_features = scaler.fit_transform(data[features])\n",
    "    \n",
    "        # Scale the target variable (kWhDelivered) separately\n",
    "        target_scaler = MinMaxScaler()\n",
    "        scaled_target = target_scaler.fit_transform(data[[target]])  # Use double brackets to keep it as a DataFrame\n",
    "        data['Scaled_kWhDelivered'] = scaled_target\n",
    "    \n",
    "        # Convert the scaled features back to a DataFrame\n",
    "        scaled_features_df = pd.DataFrame(scaled_features, columns=features, index=data.index)\n",
    "    \n",
    "        # Combine the scaled features with the scaled target variable\n",
    "        scaled_data = pd.concat([scaled_features_df, data['Scaled_kWhDelivered']], axis=1)\n",
    "    \n",
    "        # Set the sequence length (e.g., 1 for 1 months of data)\n",
    "        sequence_length = 1\n",
    "    \n",
    "        # Create sequences and targets\n",
    "        X, y = create_sequences(scaled_data, sequence_length)\n",
    "    \n",
    "        # Split the data into training and testing sets (use all except the last one for training)\n",
    "        X_train, X_test = X[:-1], X[-1:]\n",
    "        y_train, y_test = y[:-1], y[-1:]\n",
    "    \n",
    "        # Extract the months corresponding to the training and test data\n",
    "        months_train = data.index[sequence_length:len(X_train)+sequence_length]\n",
    "        months_test = data.index[-len(X_test):]\n",
    "    \n",
    "        # Define the GRU model\n",
    "        input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "        model = build_gru_model(input_shape)\n",
    "    \n",
    "        # Compile the model\n",
    "        model.compile(optimizer=Adam(learning_rate=0.0001), loss=MeanSquaredError())\n",
    "    \n",
    "        # If there is a previously trained model, load its weights for transfer learning\n",
    "        if previous_model is not None:\n",
    "            model.set_weights(previous_model.get_weights())  # Transfer weights from previous model\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)\n",
    "       \n",
    "        # Predict the values for both training and test sets\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "    \n",
    "        # Reshape y_train, y_test, y_train_pred, and y_test_pred for inverse transformation\n",
    "        y_train_scaled = y_train.reshape(-1, 1)\n",
    "        y_test_scaled = y_test.reshape(-1, 1)\n",
    "        y_train_pred_scaled = y_train_pred.reshape(-1, 1)\n",
    "        y_test_pred_scaled = y_test_pred.reshape(-1, 1)\n",
    "    \n",
    "        # Inverse transform the predictions and actual values using the target scaler\n",
    "        y_train_inversed = target_scaler.inverse_transform(y_train_scaled)\n",
    "        y_test_inversed = target_scaler.inverse_transform(y_test_scaled)\n",
    "        y_train_pred_inversed = target_scaler.inverse_transform(y_train_pred_scaled)\n",
    "        y_test_pred_inversed = target_scaler.inverse_transform(y_test_pred_scaled)\n",
    "    \n",
    "        mse = mean_squared_error(y_test_inversed, y_test_pred_inversed)\n",
    "        mae = mean_absolute_error(y_test_inversed, y_test_pred_inversed)\n",
    "        smape_value = smape(y_test_inversed, y_test_pred_inversed)\n",
    "        \n",
    "        # Store the model and metrics\n",
    "        models[stationID] = model\n",
    "        metrics[stationID] = {\n",
    "            'MAE': mae,\n",
    "            'MSE': mse,\n",
    "            'SMAPE (%)': smape_value,\n",
    "            'Actual kWh': y_test_inversed[0][0],\n",
    "            'Predicted kWh': y_test_pred_inversed[0][0]\n",
    "        }\n",
    "    \n",
    "        # Plot the actual vs predicted values against the month for both training and test sets\n",
    "        plt.figure(figsize=(12, 8))\n",
    "    \n",
    "        # Plot training data\n",
    "        plt.plot(months_train, y_train_inversed, label='Actual kWhDelivered (Training)', marker='o', color='blue')\n",
    "        plt.plot(months_train, y_train_pred_inversed, label='Predicted kWhDelivered (Training)', marker='x', color='orange')\n",
    "    \n",
    "        # Plot test data\n",
    "        plt.plot(months_test, y_test_inversed, label='Actual kWhDelivered (Test)', marker='o', color='green')\n",
    "        plt.plot(months_test, y_test_pred_inversed, label='Predicted kWhDelivered (Test)', marker='x', color='red')\n",
    "    \n",
    "        plt.title(f'Actual vs Predicted kWhDelivered (Training and Test) for {stationID}')\n",
    "        plt.xlabel('Month')\n",
    "        plt.ylabel('kWhDelivered')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "        # Update the previous model for transfer learning\n",
    "        previous_model = model\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with station {filename}: {e}\")\n",
    "        continue  # Skip to the next file if there's an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4b80dc-2c9c-4713-bda1-8b681371c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame.from_dict(metrics, orient='index').reset_index()\n",
    "\n",
    "# Rename the index column to 'stationID'\n",
    "metrics_df = metrics_df.rename(columns={'index': 'stationID'})\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce366d17-1f6b-4ce4-88b0-ac9f5acc0596",
   "metadata": {},
   "source": [
    "### Without Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdafee2-4615-40a3-b533-7af47d2c2774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the models and metrics\n",
    "models = {}\n",
    "metrics = {}\n",
    "\n",
    "# Directory containing the CSV files\n",
    "directory = \"./caltech_model_data\"  # Replace with your directory path\n",
    "\n",
    "# List all files in the directory\n",
    "filenames = sorted([filename for filename in os.listdir(directory) if filename.endswith('.csv')])\n",
    "\n",
    "# Placeholder to store the first trained model for Transfer Learning\n",
    "previous_model = None\n",
    "\n",
    "def prepare_data(features, target):\n",
    "    # For window_size = 1, each sample is its own sequence\n",
    "    return features, target\n",
    "\n",
    "def build_gru_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(LSTM(64, activation='relu', return_sequences=False))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=MeanSquaredError())\n",
    "    return model\n",
    "\n",
    "def inverse_transform(scaler, data):\n",
    "    return scaler.inverse_transform(data.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Create a lagged feature for kWhDelivered\n",
    "def create_sequences(data, sequence_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        seq = data.iloc[i:i+sequence_length][features].values\n",
    "        label = data.iloc[i+sequence_length]['Scaled_kWhDelivered']\n",
    "        sequences.append(seq)\n",
    "        targets.append(label)\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "\n",
    "\n",
    "# Loop through each CSV file in the directory\n",
    "for filename in filenames:\n",
    "    try:\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        # Load the data from the CSV file\n",
    "        data = pd.read_csv(filepath)\n",
    "        \n",
    "        # Convert the month to datetime format and set it as the index\n",
    "        data['Month'] = pd.to_datetime(data['month'])\n",
    "        data.set_index('Month', inplace=True)\n",
    "    \n",
    "        # Select the features (weather data) and target variable (kWhDelivered)\n",
    "        stationID = filename.split('.')[0]  # Example: If filename is 'station1.csv'\n",
    "        features = ['MinTemp', 'MaxTemp', 'AvgTemp', 'AvgPrecipitation', 'AvgHumidity', 'AvgWindSpeed']\n",
    "        target = 'kWhDelivered'\n",
    "    \n",
    "        # Scale the features\n",
    "        scaler = MinMaxScaler()\n",
    "        scaled_features = scaler.fit_transform(data[features])\n",
    "    \n",
    "        # Scale the target variable (kWhDelivered) separately\n",
    "        target_scaler = MinMaxScaler()\n",
    "        scaled_target = target_scaler.fit_transform(data[[target]])  # Use double brackets to keep it as a DataFrame\n",
    "        data['Scaled_kWhDelivered'] = scaled_target\n",
    "    \n",
    "        # Convert the scaled features back to a DataFrame\n",
    "        scaled_features_df = pd.DataFrame(scaled_features, columns=features, index=data.index)\n",
    "    \n",
    "        # Combine the scaled features with the scaled target variable\n",
    "        scaled_data = pd.concat([scaled_features_df, data['Scaled_kWhDelivered']], axis=1)\n",
    "    \n",
    "        # Set the sequence length (e.g., 1 for 1 months of data)\n",
    "        sequence_length = 1\n",
    "    \n",
    "        # Create sequences and targets\n",
    "        X, y = create_sequences(scaled_data, sequence_length)\n",
    "    \n",
    "        # Split the data into training and testing sets (use all except the last one for training)\n",
    "        X_train, X_test = X[:-1], X[-1:]\n",
    "        y_train, y_test = y[:-1], y[-1:]\n",
    "    \n",
    "        # Extract the months corresponding to the training and test data\n",
    "        months_train = data.index[sequence_length:len(X_train)+sequence_length]\n",
    "        months_test = data.index[-len(X_test):]\n",
    "    \n",
    "        # Define the GRU model\n",
    "        input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "        model = build_gru_model(input_shape)\n",
    "    \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)\n",
    "       \n",
    "        # Predict the values for both training and test sets\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "    \n",
    "        # Reshape y_train, y_test, y_train_pred, and y_test_pred for inverse transformation\n",
    "        y_train_scaled = y_train.reshape(-1, 1)\n",
    "        y_test_scaled = y_test.reshape(-1, 1)\n",
    "        y_train_pred_scaled = y_train_pred.reshape(-1, 1)\n",
    "        y_test_pred_scaled = y_test_pred.reshape(-1, 1)\n",
    "    \n",
    "        # Inverse transform the predictions and actual values using the target scaler\n",
    "        y_train_inversed = target_scaler.inverse_transform(y_train_scaled)\n",
    "        y_test_inversed = target_scaler.inverse_transform(y_test_scaled)\n",
    "        y_train_pred_inversed = target_scaler.inverse_transform(y_train_pred_scaled)\n",
    "        y_test_pred_inversed = target_scaler.inverse_transform(y_test_pred_scaled)\n",
    "    \n",
    "    \n",
    "        mse = mean_squared_error(y_test_inversed, y_test_pred_inversed)\n",
    "        mae = mean_absolute_error(y_test_inversed, y_test_pred_inversed)\n",
    "        smape_value = smape(y_test_inversed, y_test_pred_inversed)\n",
    "        \n",
    "        # Store the model and metrics\n",
    "        models[stationID] = model\n",
    "        metrics[stationID] = {\n",
    "            'MAE': mae,\n",
    "            'MSE': mse,\n",
    "            'SMAPE (%)': round(smape_value,2),\n",
    "            'Actual kWh': y_test_inversed[0][0],\n",
    "            'Predicted kWh': y_test_pred_inversed[0][0]\n",
    "        }\n",
    "    \n",
    "        # Plot the actual vs predicted values against the month for both training and test sets\n",
    "        plt.figure(figsize=(12, 8))\n",
    "    \n",
    "        # Plot training data\n",
    "        plt.plot(months_train, y_train_inversed, label='Actual kWhDelivered (Training)', marker='o', color='blue')\n",
    "        plt.plot(months_train, y_train_pred_inversed, label='Predicted kWhDelivered (Training)', marker='x', color='orange')\n",
    "    \n",
    "        # Plot test data\n",
    "        plt.plot(months_test, y_test_inversed, label='Actual kWhDelivered (Test)', marker='o', color='green')\n",
    "        plt.plot(months_test, y_test_pred_inversed, label='Predicted kWhDelivered (Test)', marker='x', color='red')\n",
    "    \n",
    "        plt.title(f'Actual vs Predicted kWhDelivered (Training and Test) for {stationID}')\n",
    "        plt.xlabel('Month')\n",
    "        plt.ylabel('kWhDelivered')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with station {filename}: {e}\")\n",
    "        continue  # Skip to the next file if there's an error\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a520cd-73bc-4450-88ab-51aea0d01b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame.from_dict(metrics, orient='index').reset_index()\n",
    "\n",
    "# Rename the index column to 'stationID'\n",
    "metrics_df = metrics_df.rename(columns={'index': 'stationID'})\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e6f26d-f105-4917-9a87-88ff8aea2bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
